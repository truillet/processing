# Lab2 - Processing.org avanc√©
Dans ce deuxi√®me volet, nous allons nous int√©resser √† un certain nombre de concepts et de librairies pour aller plus avant dans la programmation et les modalit√©s d‚Äôinteraction non-conventionnelles.

## de l'interaction avec "classe"
### programmation orient√©e-objet
T√©l√©charger le projet *[interface](https://github.com/truillet/upssitech/blob/master/SRI/1A/Code/Gestion_Objets.zip)* et modifier le code de telle mani√®re que l‚Äôobjet change de couleur quand on clique dessus et revienne √† sa couleur initiale quand on le rel√¢che (penser √† utiliser les √©v√©nements *MouseDragged()*, *MousePressed()* et *MouseReleased()*).

### une machine *dans tous ses √©tats*
Une mani√®re efficace de concevoir des programmes interactifs est d‚Äôutiliser une machine √† √©tats. Le programme passe d‚Äô√©tats en √©tats gr√¢ce √† des transitions qui sont la plupart du temps des √©v√©nements provenant soit de l‚Äôutilisateur (actions de la souris, sur le clavier, ‚Ä¶), soit du syst√®me lui-m√™me.

La fonction **draw()** va finalement ressembler √† la structure suivante, proposant diff√©rentes visualisations suivant l‚Äô√©tat courant :
```
FSM mae // Machine A Etats

‚Ä¶

void draw() {
	switch(mae) {
		case INITIAL :
		‚Ä¶
		break;
	‚Ä¶
		default:
			‚Ä¶
			break;
}
}
```

A partir de l‚Äôexemple fourni ici	 (https://github.com/truillet/upssitech/blob/master/SRI/1A/Code/Machine_Etats.zip), √©crire un programme qui d√©marre la webcam / arr√™te la webcam quand l‚Äôutilisateur tape sur la barre espace. (Par d√©faut, une image de renardeau (tout mignon) sera affich√©e √† la place du flux vid√©o)

## la mise en r√©seau des donn√©es
### les donn√©es JSON
Sur https://api.nasa.gov, g√©n√©rez une nouvelle cl√© API

A l‚Äôaide de l‚ÄôAPI APOD (Astronomy Picture Of the Day) et JSONObject	 (https://processing.org/reference/JSONObject.html), √©crivez un sketch Processing qui r√©cup√®re et affiche la photo du jour dans une fen√™tre (taille de la fen√™tre adapt√©e √† la taille de la photo) et affiche le titre de l‚Äôimage du jour dans la barre sup√©rieure de la fen√™tre Processing.

## un peu de R√©alit√© Augment√©e
### √† base de QRCode
A partir de l‚Äôexemple t√©l√©charg√© ici :	 https://github.com/truillet/upssitech/blob/master/SRI/1A/Code/QRCode.zip
Cr√©er une application qui affiche un objet 3D sur le QR code d√©tect√©.

Nota : cette application utilise la librairie ZXing (https://github.com/zxing/zxing)

Nota2 : Pour g√©n√©rer un QR Code depuis Processing, voir le code ici : https://github.com/truillet/upssitech/blob/master/SRI/1A/Code/QRCode_Generator.zip

### √† base de TopCodes
A partir de l‚Äôexemple t√©l√©charg√© ici : https://github.com/truillet/TopCode
Cr√©er une application qui affiche des informations sur des objets physiques √©quip√©s de TopCodes rep√©r√©s par une webcam quand l‚Äôutilisateur clique sur l‚Äôobjet.

Nota : cette application utilise la librairie TopCodes-Tangible Objet Placement Codes r√©√©crite pour Processing (voir http://users.eecs.northwestern.edu/~mhorn/topcodes pour la version originale)

### BoofCV
Installer au pr√©alable la librairie BoofCV for Processing disponible dans le menu Outils | Ajouter un outil‚Ä¶ puis onglet Libraries. BoofCV (https://boofcv.org) est un ensemble de fonctions de manipulation d‚Äôimages.

Aller dans Fichier | Exemples‚Ä¶ Dans la fen√™tre ouverte (Java Examples), aller dans le sous-r√©pertoire Contributed Libraries |BoofCVforProcessing ouvrez le sketch Fiducials. L‚Äôouvrir et l‚Äôex√©cuter. 
Modifier le code de telle mani√®re que quand le pattern fiduciaire est d√©tect√© devant la cam√©ra, un texte cod√© apparait √† l‚Äô√©cran sur le pattern.

Fiducial √† t√©l√©charger :	 https://github.com/truillet/upssitech/blob/master/SRI/1A/TP/fiducial_BoofCV.pdf (voir la documentation ici : http://boofcv.org/index.php?title=Tutorial_Fiducials)

Nota : Vous pouvez aussi essayer les autres exemples (TrackingObject ou RemovePerspective par exemple üòâ) int√©ressants pour d‚Äôautres sujets √† traiter (comme le suivi d‚Äôobjet en temps-r√©el ou le changement de perspective d‚Äôun objet).


### NyARToolkit
NyARToolkit (https://nyatla.jp/nyartoolkit) est une version modifi√©e de la c√©l√®bre librairie ARToolkit d√©velopp√©e par l‚Äôuniversit√© de Washington il y a une vingtaine d‚Äôann√©e	 (http://www.hitl.washington.edu/artoolkit).

T√©l√©charger la librairie NyARToolkit pour Processing.org ici : https://github.com/nyatla/NyARToolkit-for-Processing
D√©zipper le fichier nyar4psg.zip et place le r√©pertoire (nyar4pasg) dans le r√©pertoire libraries emplacement de sketchbook (ex : C:\dev\processing). Relancer Processing.org pour que le changement soit pris en compte.

Aller dans Fichier | Exemples‚Ä¶ Dans la fen√™tre ouverte (Java Examples), aller dans le sous-r√©pertoire Contributed Libraries|nyar4psg et ouvrez le sketch multimarker. 
A partir de l‚Äôexemple, d√©velopper une application qui permet de s√©lectionner une zone film√©e par la webcam et prend une photo (sauv√©e en jpeg) quand on appuie sur la barre espace.

Nota : si vous voulez ne pas utiliser de pattern ARToolkit, vous pouvez cr√©er le v√¥tre √† partir d‚Äôune image gr√¢ce √† l‚Äôexemple Fichier | Exemples‚Ä¶ Dans la fen√™tre ouverte (Java Examples), aller dans le sous-r√©pertoire Contributed Libraries|nyar4psg et ouvrez le sketch nftFilesGen (Natural Feature Tracker). Ouvrir ensuite dans le m√™me r√©pertoire le fichier simpleNft en le modifiant avec le motif que vous venez de cr√©er (modifier la ligne 22)

Nota 2 : les patterns ARToolkit kanji et hiro sont t√©l√©chargeables ici :	 https://niebert.github.io/SamplesAR/markers/Hiro_Kanji_3Markers.pdf

## un peu de distribution et de multimodalit√©
### la classe Robot
La classe Robot de java (import java.awt.Robot) permet de prendre la main sur l‚Äôensemble du bureau (Windows, MacOS ou Linux) y compris hors de la fen√™tre en simulant les appuis souris et/ou au clavier.
Ecrire un sketch Processing.org qui permet de prendre un screenshot de l‚Äô√©cran et le sauvegarder au format jpeg.

### le bus logiciel ivy
T√©l√©charger l‚Äôexemple ici	 https://github.com/truillet/upssitech/blob/master/SRI/1A/Code/ivyP5.zip

D√©zipper les deux sketchs et lancer-les tous les deux. Ces deux sketchs utilisent le middleware ivy pour communiquer sur le r√©seau local (voir https://github.com/truillet/ivy pour une information g√©n√©rale).

En cliquant sur la premi√®re fen√™tre (sketch ¬´ sender ¬ª), un message sera affich√© sur l‚Äôautre fen√™tre (¬´ receiver ¬ª) et un feedback est envoy√© √† la premi√®re.

Une fois compris le principe, √©crire une interface compos√©e de plusieurs sketchs (qui peuvent communiquer sur le r√©seau local) qui d√©code un QR-code pr√©sent√© devant une cam√©ra, affiche l‚Äôinformation d√©cod√©e dans une autre fen√™tre (d‚Äôune autre machine par exemple) et lit via une synth√®se vocale le texte d√©cod√© (on pourra utiliser la librairie ttslib - https://www.local-guru.net/blog/pages/ttslib - par exemple).

## pour finir de mani√®re un peu HARD(ware)
Utiliser au pr√©alable l‚ÄôIDE Arduino (https://www.arduino.cc) sur votre machine.

T√©l√©charger l‚Äôexemple ici :	 https://github.com/truillet/upssitech/blob/master/SRI/1A/Code/processing_arduino.zip

Compiler et t√©l√©verser le code capteur.ino sur le module arduino branch√© sur le port s√©rie. Brancher une led infrarouge sur le pin Analogique A0 et GND. (le code va lire la valeur du capteur et l‚Äô√©crire sur le port s√©rie)

Ex√©cuter le code Processing.org. 	
Modifier le code de telle mani√®re que la valeur du capteur r√©cup√©r√©e soit affich√©e sous forme de barre verticale entre 0 (si la valeur r√©cup√©r√©e est ¬´ 0 ¬ª) et 300 pixels maximum (si la valeur r√©cup√©r√©e est ¬´ 1024 ¬ª)

